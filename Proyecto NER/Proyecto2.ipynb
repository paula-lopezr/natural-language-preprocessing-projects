{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620d6b10",
   "metadata": {},
   "source": [
    "# Proyecto 2: De clasificación a NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e2eee0",
   "metadata": {},
   "source": [
    "En este proyecto utilizarán [este dataset](https://archive.ics.uci.edu/ml/datasets/Paper+Reviews) que consiste en evaluaciones de diferentes revisores sobre papers (405). Cada evaluación ha sido etiquetada con 5 clases (muy negativo, negativo, 0, neutral, positivo y muy positivo). Este proyecto consiste en realizar una clasificación de este dataset y además identificar que parte de los comentarios hacen referencia a las clases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f232680",
   "metadata": {},
   "source": [
    "## Parte 1. Clasificación sencilla de cada revisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9afac9",
   "metadata": {},
   "source": [
    "En esta parte tendrá que hacer un modelo de clasificación usando redes recurrentes (LSTM, GRU), para los cinco tipos de categorias.  Recuerde algunos pasos para realizar la clasificación:\n",
    "\n",
    "1. Lectura de los datos (división en training/validation sets)\n",
    "2. Preprocesamiento del texto (no es necesario ser tanto preprocesamiento como en word2vec, pero si eliminar algunos carácteres o incluso revisar si hay comentarios vacíos). \n",
    "3. Creación del vocabulario y transformación de palabras a índices (y viceversa)\n",
    "4. Creación del \"dataset\" y \"dataloader\" involucrando los pasos 2 y 3.\n",
    "5. Creación del modelo, involucrando embeddings.\n",
    "6. Entrenar y validar (escoger optimizador Adam o AdamW, función de costo apropiada, loops de entrenamiento y validación).\n",
    "7. Modelo listo para producción (modelo para predecir entrando un comentario)\n",
    "8. Pequeña interfaz para predicción usando Gradio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb049d",
   "metadata": {},
   "source": [
    "Realice diferentes configuraciones, tamaño del embedding, arquitecturas LSTM o GRU, varias capas, bidireccionalidad, etc. Escoja el mejor modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470433ed",
   "metadata": {},
   "source": [
    "## Parte 2. Convirtiendo el texto a un problema de clasificación por palabras o frases (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc2d904",
   "metadata": {},
   "source": [
    "## Parte 2 A. Etiquetado \n",
    "Etiquetado de los comentarios frases a frases\n",
    "Usando herramientas como [brat](https://brat.nlplab.org/), [docanno](https://doccano.herokuapp.com/) o [inception](https://inception-project.github.io/) (ver [este blog](https://dida.do/blog/the-best-free-labeling-tools-for-text-annotation-in-nlp) para un rápido resumen sobre estas aplicaciones), deberán etiquetar las frases en los comentarios con las etiquetas que correspondan a cada categoría. Un ejemplo para el primer comentario lo muestra la siguiente figura: \n",
    "![image](ejemploetiquetado.png)\n",
    "\n",
    "Nota: El anterior ejemplo lo hice en la herramienta de inception (y exportando el archivo a WebAnno TSV v3.3).\n",
    "Como pueden observar la mayoría de palabras no se etiquetarán (pertenecen a una categoría como \"otros\").\n",
    "\n",
    "No deben etiquetar todo el dataset, pueden hacerlo con unas 50 (por grupo). Pero pueden intercambiarse (entre grupos) los datasets etiquetados para incrementar el número de datos (para esto es recomendable usar la misma herramienta y el mismo tipo de archivo de exportación)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023749c",
   "metadata": {},
   "source": [
    "### Parte 2. Clasificación y localización de las frases\n",
    "Cree un modelo de clasificación por palabra (es similar al punto 2 del taller 4), donde involucren redes recurrentes. Cuando varias palabras contiguas pertenezcan a la misma etiqueta deberan unirlas para etiquetarlas como frase.  Para este modelo deben seguir los mismos pasos de la parte 1, pero cambiando un poco el modelo. \n",
    "\n",
    "- Desarrolle un modelo inicializando los pesos de forma aleatoria (por defecto)\n",
    "- Desarrolle un modelo partiendo de los pesos de la parte 1.\n",
    "- Desarrolle un modelo word2vec y posteriormente cargue estos pesos en la capa de embedding para desarrollar el modelo recurrente.Les puede ser útil [esto](https://gist.github.com/dhruvdcoder/cbb8d7967a499ba85418c18414e2cdce).\n",
    "- Compare los modelos anteriores.\n",
    "- Visualice la clasificación de algunas frases del conjunto de validación usando spacy o la herramienta que usaron para etiquetar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863d8fb",
   "metadata": {},
   "source": [
    "-----\n",
    "Es importante que realicen gráficos y visualizaciones que ayuden a la interpretación. No olviden ir analizando y comentando los hallazgos, y sobretodo **concluir**. El entregable es un notebook de Jupyter, debidamente presentado y comentado.\n",
    "\n",
    "- ¿Qué diferencian encuentran con un tipo de clasificación respecto al otro?\n",
    "- ¿Los pesos de una tarea ayudan a la otra?\n",
    "- ¿Es posible analizar o intuir a partir de pesos o activaciones, que parte del texto está ayudando en la parte 1 a clasificar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f9c484",
   "metadata": {},
   "source": [
    "## Parte 3 [Opcional]. Modelo de Generación de revisiones\n",
    "\n",
    "Esta parte es totalmente opcional, no puntua en si para el proyecto (pero si puede dar bonificación).\n",
    "En esta parte debera crear un modelo de generación de texto a partir del corpus completo de las revisiones. Esto es lo que se conoce como un modelo de generación del texto. La idea es usar modelos recurrentes para generar texto. Básicamente es crear un modelo que pueda predecir las palabras del corpues teniendo en cuenta los estados anteriores (los tokens anteriores).  Se podría por ejemplo iniciar con una frase y posteriormente producir una palabra, luego se une a la frase y se sigue así de forma sucesiva. Si desean hacer esta parte, [este blog](https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/) y [este otro blog](https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html) les puede servir de ayuda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce15703",
   "metadata": {},
   "source": [
    "### Tip de lectura de datos del archivo JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3c19c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-16T12:33:33.939699Z",
     "iopub.status.busy": "2022-03-16T12:33:33.939458Z",
     "iopub.status.idle": "2022-03-16T12:33:34.154152Z",
     "shell.execute_reply": "2022-03-16T12:33:34.153830Z",
     "shell.execute_reply.started": "2022-03-16T12:33:33.939633Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0894624",
   "metadata": {},
   "source": [
    "Leemos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e5ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('reviews.json', encoding=\"utf8\")\n",
    "data = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "032fc598",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence</th>\n",
       "      <th>evaluation</th>\n",
       "      <th>id</th>\n",
       "      <th>lan</th>\n",
       "      <th>orientation</th>\n",
       "      <th>remarks</th>\n",
       "      <th>text</th>\n",
       "      <th>timespan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>- El artículo aborda un problema contingente y...</td>\n",
       "      <td>2010-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>El artículo presenta recomendaciones prácticas...</td>\n",
       "      <td>2010-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>- El tema es muy interesante y puede ser de mu...</td>\n",
       "      <td>2010-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>Se explica en forma ordenada y didáctica una e...</td>\n",
       "      <td>2010-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2010-07-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  confidence evaluation  id lan orientation remarks  \\\n",
       "0          4          1   1  es           0           \n",
       "1          4          1   2  es           1           \n",
       "2          5          1   3  es           1           \n",
       "3          4          2   1  es           1           \n",
       "4          4          2   2  es           0           \n",
       "\n",
       "                                                text    timespan  \n",
       "0  - El artículo aborda un problema contingente y...  2010-07-05  \n",
       "1  El artículo presenta recomendaciones prácticas...  2010-07-05  \n",
       "2  - El tema es muy interesante y puede ser de mu...  2010-07-05  \n",
       "3  Se explica en forma ordenada y didáctica una e...  2010-07-05  \n",
       "4                                                     2010-07-05  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.json_normalize(data['paper'], record_path = [\"review\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "978ad4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData(Dataset):\n",
    "    '''\n",
    "    Dataset basico para leer los datos de tweets\n",
    "    '''\n",
    "    def __init__(self, filename):\n",
    "        super(TextData, self).__init__()\n",
    "        self.df = df[[\"evaluation\", \"text\"]]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.df.iloc[index,0], self.df.iloc[index,1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1ac510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    new_str = emoji_pattern.sub(r'[]', string)\n",
    "    return re.sub(r'[^\\w\\s]', '', new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b134a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    ps = PorterStemmer()\n",
    "    token = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    list_text = token.tokenize(text)\n",
    "    list_text = [x.lower() for x in list_text]\n",
    "    list_text = [ps.stem(x) for x in list_text]\n",
    "    list_text = [remove_emoji(x) for x in list_text]\n",
    "    return list_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c01acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df['text'] == \"\"].index, axis=0)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "df['evaluation'] = df['evaluation'].apply(lambda x: int(x)+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "200592ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TextData(df)\n",
    "train_dataset, valid_dataset = random_split(ds,\n",
    " [int(len(df)*0.7),len(df) - int(len(df)*0.7)], torch.manual_seed(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557189b5",
   "metadata": {},
   "source": [
    "Limpiamos y tokenizamos los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170af46b",
   "metadata": {},
   "source": [
    "Creamos el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b4a4fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab-size: 4773\n"
     ]
    }
   ],
   "source": [
    "token_counts = Counter()\n",
    "\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    " \n",
    "    \n",
    "print('Vocab-size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1573f0b",
   "metadata": {},
   "source": [
    "Tranformamos las palabras a índices (y viceversa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4825343",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "vocab = vocab(ordered_dict)\n",
    "\n",
    "vocab.insert_token(\"<pad>\", 0)\n",
    "vocab.insert_token(\"<unk>\", 1)\n",
    "vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9802f634",
   "metadata": {},
   "source": [
    "Creación del \"dataset\" y \"dataloader\" involucrando los pasos 2 y 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaad8db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paso 3-A: Defina las funciones para la transformación.\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1. if x == 'pos' else 0.\n",
    "\n",
    "\n",
    "## Paso 3-B: función de codificación y transformación\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), \n",
    "                                      dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
    "        text_list, batch_first=True)\n",
    "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47f2dc4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  29,   73,  117,  714, 1780,  204, 1111,   11, 2523, 2524,   75,  641,\n",
      "           29,   73,  922,  310, 2525,  118,   29,  332,  642, 2526,   29, 1349,\n",
      "         1781,  117, 2527,  204, 1112,   29,   73, 1350,  118, 1782,  114, 1111,\n",
      "         2528,   11, 2529,   29,   73,  147, 2530,  431,  466, 1112,   29,   73,\n",
      "          597,  923,   11, 1783, 2531, 1784,   75,   29, 1113,  374,  310, 2532,\n",
      "         1351, 1352,   10, 1114,  467, 2533, 1353, 1115, 1113,  141,  310, 2534,\n",
      "         1785,  643,  283, 2535, 1786,  148,  504,  110, 1787,  374,  310, 2536,\n",
      "         1788, 1354,  204,  924,  352, 1116,  118,  311, 1789,  504,  925,   11,\n",
      "         1355, 2537,  468, 1356,  114, 1117, 2538,  333, 1357, 2539,  117, 1116,\n",
      "         1790, 2540,  548, 2541,  808, 1791,  217, 1356,  114, 1117,  117,  310,\n",
      "         1358,  227,   11, 2542, 2543,   75, 1792,  114, 2544, 2545,   11, 2546,\n",
      "         1359,  467,  117, 1793,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  13,   15,   39,   42,  151,    6,   56,  334,   21,  926,    6,   10,\n",
      "           35, 1794,   19,    9,    6,    4,  152,   57,   26, 2547,  284,    9,\n",
      "           28,   14,   40,  270,   26, 2548,   10,  399,   16, 1795,  927,  598,\n",
      "          928,    9,  926,   61,  178, 2549,    5,  153,  125,    2,  929, 1360,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   4,   17,  126,    4,   17,  262,   16,    3,  469,    2,   15,   39,\n",
      "          252, 1118,   11,   15, 1361,  253, 1362,    3,  165,   14,   17,  262,\n",
      "           10,   13, 1363,    8, 1364,  335,  470,   45,   24,  166,   34,    2,\n",
      "          353,  505,    7,  105,    3,   77, 1365,    8, 2550,   41,   80,   10,\n",
      "            7,  205,    3, 2551,    2,  285,    8,    4,   84,  354,   23,   17,\n",
      "           30,  715,    2,    3,   43,   37, 1366,    7,  197,    3,  292,    2,\n",
      "         1367,   16,  930,  931,    2, 2552,   52,   10,    7, 1368,  205,  506,\n",
      "            2,    3,  189,   33,   61,  137, 2553,    2,    3,   33,  507,   10,\n",
      "            7,  167,    4,   72,    2,    3,   63,    2, 1119,  932,  933,   19,\n",
      "            9,  198,   90,  312,   15,  400,    2, 1369,    7,  197,    3,  292,\n",
      "           14,  599, 1796,    8, 2554,   21,  129,    2,   47,   41,   80,   10,\n",
      "            7,  644,  205,    2,    3,  166,  240,  375, 1120, 2555,  148,   10,\n",
      "          126,  206,    2,    3,  166,    2,    3,   39,  432,    2,  934,  432,\n",
      "            2,  129,    2,   47, 1797,  375,  222, 2556, 2557, 2558,  148,   34,\n",
      "            2,   68,  935,    5,    4,   17,    3,  936,    2, 1798,   10,   35,\n",
      "           87,    8, 2559,  228,  293,   23,  213,   21,  471,    3, 1119,    2,\n",
      "           12, 1362,    3,   37,  263,    7,  248, 2560,   45,   89,    4,   81,\n",
      "           13, 1370,    3,   37,  508,  137,   25,   11,    3,   39,  716,  809,\n",
      "            8,   10,    7,  472,  190,    3, 1799,   20,    3,   39],\n",
      "        [  27,   18,   31,    4,   36,    2,   12,   40,    2,  549,    2, 1371,\n",
      "         1800, 1801,    8, 2561,  313,   91,    2, 1121,  376,    5,    4,  168,\n",
      "         1802,    5, 1803,    2,  249,    2,  110, 1372,    4,   18,    7,   31,\n",
      "           87,    5,   24,   78,    8,    7, 2562,    4,   88,    2,  154, 2563,\n",
      "         2564,   26,   12,   18,   42,   16,   26,  139,    5, 1122,  158,  214,\n",
      "          473, 2565,   35,   37,  263,   31,  433,   20, 1803,    2,  294, 1372,\n",
      "          645,    6,    4,    3,   33,  140,  110, 1372,   11,  214,  107,    4,\n",
      "           40,   10,  937,   12,   38,    2,  434,    2,   47,  600,   11,    6,\n",
      "           10,   56,  271,  355,  938,  356,   30,   48,   13,   12,   40,    2,\n",
      "          601,   62,    6, 1804,  271,  355,   52,    4,   85,   10, 1805,   21,\n",
      "            4,   40,  169, 2566,   11,  401,  271,  355,    2,    9, 1371,  207,\n",
      "          550,  602,    8,  115,    2,  646,   16, 2567,    8,  647,    4,   82,\n",
      "            2, 1802,   45,   69,   94,    2, 1121,   22,  648,    5,  336,    2,\n",
      "           12, 1806,    7, 2568,    6,   12, 1371, 1800, 1801, 1373,   12,  314,\n",
      "         1807,    2, 1808,    5,   12,  939,  551,   19,    9,    6,   10,  123,\n",
      "           87,   53,   13,  286,   12,   40,  552, 2569,   16, 1123,   27, 1809,\n",
      "           61,  218,   26, 1810,   53,    7, 2570,    3,  509,   20,   69,  229,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([136,  48, 226, 204])\n",
      "torch.Size([4, 226])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)\n",
    "text_batch, label_batch, length_batch = next(iter(dataloader))\n",
    "print(text_batch)\n",
    "print(label_batch)\n",
    "print(length_batch)\n",
    "print(text_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2573791d",
   "metadata": {},
   "source": [
    "Creación del modelo, involucrando embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef0f43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203fd41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, \n",
    "                                      embed_dim, \n",
    "                                      padding_idx=0) \n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "         \n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227ad0b",
   "metadata": {},
   "source": [
    "Funciones de entreanmiento y evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3dbaabf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
    " \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item()*label_batch.size(0)\n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "866892e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28876/1479824609.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0macc_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0macc_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28876/1544400607.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtext_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "loss_fn =nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10 \n",
    "\n",
    "torch.manual_seed(1)\n",
    " \n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dataloader)\n",
    "    acc_valid, loss_valid = evaluate(test_dataloader)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37b39c",
   "metadata": {},
   "source": [
    "Entrenar y validar (escoger optimizador Adam o AdamW, función de costo apropiada, loops de entrenamiento y validación)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7952e516",
   "metadata": {},
   "source": [
    "7. Modelo listo para producción (modelo para predecir entrando un comentario)\n",
    "8. Pequeña interfaz para predicción usando Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bbb7529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Etiquetar de 250 a 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf4839",
   "metadata": {},
   "source": [
    "### Parte 2. Clasificación y localización de las frases\n",
    "Cree un modelo de clasificación por palabra (es similar al punto 2 del taller 4), donde involucren redes recurrentes. Cuando varias palabras contiguas pertenezcan a la misma etiqueta deberan unirlas para etiquetarlas como frase.  Para este modelo deben seguir los mismos pasos de la parte 1, pero cambiando un poco el modelo. \n",
    "\n",
    "- Desarrolle un modelo inicializando los pesos de forma aleatoria (por defecto)\n",
    "- Desarrolle un modelo partiendo de los pesos de la parte 1.\n",
    "- Desarrolle un modelo word2vec y posteriormente cargue estos pesos en la capa de embedding para desarrollar el modelo recurrente.Les puede ser útil [esto](https://gist.github.com/dhruvdcoder/cbb8d7967a499ba85418c18414e2cdce).\n",
    "- Compare los modelos anteriores.\n",
    "- Visualice la clasificación de algunas frases del conjunto de validación usando spacy o la herramienta que usaron para etiquetar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "251093f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "79e26ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =  df['text'][250:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "45886485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] if w in to_ix else to_ix[\"<unk>\"] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def quitartildes(s):\n",
    "    s = re.sub(\n",
    "            r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", \n",
    "            normalize( \"NFD\", s), 0, re.I\n",
    "        )\n",
    "    return normalize( 'NFC', s)\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    return quitartildes(sentence).lower().split()\n",
    "\n",
    "train_sentences = [preprocess_sentence(sent) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8e53780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent in train_sentences:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33a715ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c01c84cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7ec23df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 20\n",
    "HIDDEN_DIM = 10\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
