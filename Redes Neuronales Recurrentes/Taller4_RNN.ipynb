{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgXRctaHolTG"
      },
      "source": [
        "# Taller 4: Redes Neuronales Recurrentes\n",
        "\n",
        "**Jessenia Piza, Laura Alejandra Salazar & Paula Lorena López**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W8Oim60olTQ"
      },
      "source": [
        "Este taller consiste en actualizar el taller anterior (3), es decir, usar redes neuronales recurrentes, en vez de redes neuronales multicapa (MLP, multilayer perceptron)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xibvTibbolTR"
      },
      "source": [
        "## Ejercicio 1. \n",
        "Use el dataset de `progressive-tweet-sentiment.csv` (el del taller 1 y taller 3), para realizar una clasificación de las 4 clases en que pertenece cada tweet, usando redes neuronales recurrentes. Es decir, que esta vez, no va a usar sólo capas MLP, sino una o varias capas recurrentes, seguida de una capa lineal (full connected, o densa). Pruebe diferentes configuraciones (use recurrente básica, LSTM, GRU y bidireccionalidad). \n",
        "\n",
        "Debe utilizar pytorch. Grafique el loss y el accuracy, tanto para el entrenamiento como para la validación. Escoja el mejor modelo probando con un buen número de epochs (use el optimizador de Adam con learning rate por defecto de 0.001).\n",
        "\n",
        "Recuerde que en este ejercicio no va a usar word2vec, sino una capa de embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBJNXFzFolTU"
      },
      "source": [
        "A continuación muestro como leer el conjunto de datos, crear un `Dataset`de pytorch y dividir en un training y validation set (si se quiere un test se puede repetir el proceso)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BVE_13S2olTV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "import torch.nn as nn\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from torchtext.vocab import vocab\n",
        "from collections import Counter, OrderedDict\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lv3OoOprolTp"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from unicodedata import normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se tiene una función que lee la base de  datos con la que se trabajará.\n",
        "Se guarda y se divide en `train_dataset` y `valid_dataset`.\n"
      ],
      "metadata": {
        "id": "Ol3Cgmk28mZy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QZAasfadolTZ"
      },
      "outputs": [],
      "source": [
        "class TextData(Dataset):\n",
        "    '''\n",
        "    Dataset basico para leer los datos de tweets\n",
        "    '''\n",
        "    def __init__(self, filename):\n",
        "        super(TextData, self).__init__()\n",
        "        df = pd.read_csv(filename,encoding='latin-1')\n",
        "        self.df = df[[\"target\", \"tweet\"]]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.df.iloc[index,0], self.df.iloc[index,1]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GF4mauSuolTb"
      },
      "outputs": [],
      "source": [
        "ds = TextData(\"progressive-tweet-sentiment.csv\")\n",
        "train_dataset, valid_dataset = random_split(ds,\n",
        " [int(len(ds)*0.7),len(ds) - int(len(ds)*0.7)], torch.manual_seed(42))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez hecho esto, se limpían todos los tweets con los que se trabajarán, de manera que queda más fácil el análisis de este."
      ],
      "metadata": {
        "id": "zhLIQFLP81G4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LKrE-UF6olTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d3d106-df79-4b99-e711-be2b35267cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab-size: 3785\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "token_counts = Counter()\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
        "        ' '.join(emoticons).replace('-', '')\n",
        "    tokenized = text.split()\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "for label, line in train_dataset:\n",
        "    tokens = tokenizer(line)\n",
        "    token_counts.update(tokens)\n",
        " \n",
        "    \n",
        "print('Vocab-size:', len(token_counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DgShCNypolTf"
      },
      "outputs": [],
      "source": [
        "sorted_by_freq_tuples = sorted(token_counts.items(),\n",
        "                               key=lambda x: x[1], reverse=True)\n",
        "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "\n",
        "vocab = vocab(ordered_dict)\n",
        "\n",
        "vocab.insert_token(\"<pad>\", 0)\n",
        "vocab.insert_token(\"<unk>\", 1)\n",
        "vocab.set_default_index(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De esta manera, ya tenemos un vocaulario creado.\n"
      ],
      "metadata": {
        "id": "kD4Jw_xg-WM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se definen las funciones para la transformación y de esta manera, se aplican para la función de codificación y transformación. \n",
        "\n",
        "Esto lo que permite es retornar el texto de su respectivo batch con su padding y lables."
      ],
      "metadata": {
        "id": "9aUp0cbK-mb6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-CUgHIR5olTi"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "lbl = {'Feminist Movement':0, 'Hillary Clinton':1,\n",
        "       'Legalization of Abortion':2, 'Atheism': 3}\n",
        "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
        "label_pipeline = lambda x: lbl[x]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, lengths = [], [], []\n",
        "    for _label, _text in batch:\n",
        "        label_list.append(label_pipeline(_label))\n",
        "        processed_text = torch.tensor(text_pipeline(_text), \n",
        "                                      dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        lengths.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list)\n",
        "    lengths = torch.tensor(lengths)\n",
        "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
        "        text_list, batch_first=True)\n",
        "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)"
      ],
      "metadata": {
        "id": "5Dun0Jv808r2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea el Dataloader de train y de valid. \n",
        "Se utiliza un batch pequeño dado que la longitud de los tweets no es tan larga."
      ],
      "metadata": {
        "id": "Iy9O1Sci_Kpg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oVfFEzNSolTl"
      },
      "outputs": [],
      "source": [
        "batch_size = 40\n",
        "train_dl = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                      shuffle=True, collate_fn=collate_batch)\n",
        "valid_dl = DataLoader(valid_dataset, batch_size=batch_size,\n",
        "                      shuffle=False, collate_fn=collate_batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea la clase de la Red Neuronal Recurrente.\n",
        "Se tabaja con una RNN bidereccional. Esto quiere decir que la capa recurrente es bidireccional. Así que pasa por vaarias capas entre ella están _embedding, LSTM, Linear_.\n",
        "\n",
        "El tamaño del batch es el mismo que para la construcción del Dataloader."
      ],
      "metadata": {
        "id": "8s-6-3j1_ihh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rgnDiNecolTm"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, \n",
        "                                      embed_dim, \n",
        "                                      padding_idx=0) \n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, \n",
        "                           batch_first=True)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(fc_hidden_size, 4)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text, lengths):\n",
        "        out = self.embedding(text)\n",
        "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(),\n",
        "                                                enforce_sorted=False,\n",
        "                                                batch_first=True)\n",
        "        out, (hidden, cell) = self.rnn(out)\n",
        "        out = hidden[-1, :, :]\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "embed_dim = 25\n",
        "rnn_hidden_size = 64\n",
        "fc_hidden_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size) \n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "VA7vOk7f1bcR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crean las funciones de entrenamiento y evaluación del modelo."
      ],
      "metadata": {
        "id": "_a5pTWDdBUJO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hovW9Y6TolTn"
      },
      "outputs": [],
      "source": [
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_loss = 0, 0\n",
        "    for text_batch, label_batch, lengths in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(text_batch, lengths)\n",
        "        loss = loss_fn(pred, label_batch.type(torch.LongTensor))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        var = (torch.argmax(pred, dim=1)).float()\n",
        "        total_acc += (var == label_batch).float().sum().item()\n",
        "        total_loss += loss.item()*label_batch.size(0)\n",
        "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
        " \n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_loss = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for text_batch, label_batch, lengths in dataloader:\n",
        "            pred = model(text_batch, lengths)\n",
        "            loss = loss_fn(pred, label_batch.type(torch.LongTensor))        \n",
        "            var = (torch.argmax(pred, dim=1)).float()\n",
        "            total_acc += (var == label_batch).float().sum().item()\n",
        "            total_loss += loss.item()*label_batch.size(0)\n",
        "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se utiliza `CrossEntropyLoss` como función de pérdida y el optimizador de `Adam` para evaluar el modelo. \n",
        "Además se utilizan 10 épocas para el entrenamiento."
      ],
      "metadata": {
        "id": "mChe90joEFCd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLFfNwRHolTo",
        "outputId": "e01eccbc-e472-44d6-eca4-f2a41708043c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 1.3845 val_loss: 1.3830 accuracy: 0.2713 val_accuracy: 0.2701\n",
            "Epoch 1 loss: 1.3805 val_loss: 1.3790 accuracy: 0.2935 val_accuracy: 0.2845\n",
            "Epoch 2 loss: 1.3722 val_loss: 1.3650 accuracy: 0.3255 val_accuracy: 0.3046\n",
            "Epoch 3 loss: 1.3455 val_loss: 1.3411 accuracy: 0.4192 val_accuracy: 0.3362\n",
            "Epoch 4 loss: 1.2912 val_loss: 1.3089 accuracy: 0.4439 val_accuracy: 0.3851\n",
            "Epoch 5 loss: 1.2272 val_loss: 1.2803 accuracy: 0.5216 val_accuracy: 0.4138\n",
            "Epoch 6 loss: 1.1781 val_loss: 1.2512 accuracy: 0.5832 val_accuracy: 0.4483\n",
            "Epoch 7 loss: 1.1239 val_loss: 1.2422 accuracy: 0.6301 val_accuracy: 0.4626\n",
            "Epoch 8 loss: 1.0934 val_loss: 1.2443 accuracy: 0.6523 val_accuracy: 0.4598\n",
            "Epoch 9 loss: 1.0535 val_loss: 1.2199 accuracy: 0.6954 val_accuracy: 0.4885\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10 \n",
        "\n",
        "torch.manual_seed(1)\n",
        "list_loss_train = []\n",
        "list_loss_valid = []\n",
        "list_acc_train = []\n",
        "list_acc_valid = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    acc_train, loss_train = train(train_dl)\n",
        "    acc_valid, loss_valid = evaluate(valid_dl)\n",
        "    list_loss_train.append(loss_train)\n",
        "    list_loss_valid.append(loss_valid)\n",
        "    list_acc_train.append(acc_train)\n",
        "    list_acc_valid.append(acc_valid)\n",
        "    print(f'Epoch {epoch} loss: {loss_train:.4f} val_loss: {loss_valid:.4f} accuracy: {acc_train:.4f} val_accuracy: {acc_valid:.4f}')\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJyjSUa3olTp"
      },
      "source": [
        "## Ejercicio 2.\n",
        "Ahora proceda a actualizar el ejercicio 5 del taller 3, pero usando RNNs. Es decir, debe solucionar un problema de Name Entity Recognition (NER), con un dataset pequeño creado por ustedes (con varias entidades), pero usando redes recurrentes (RNN básica, LSTM, GRU). Use GRU o LSTM.\n",
        "\n",
        "Este ejercicio es en realidad más simple que el del anterior taller, ya que no hay necesidad de organizar las sentencias centradas en cada palabra, sino que se toma directamente cada frase (o secuencia). \n",
        "\n",
        "Lo importante ahora, es utilizar todos los estados de salida de la red recurrente y llevarlos a una capa lineal de clasificación. \n",
        "\n",
        "[Este tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) le puede ser muy útil para desarrollar este punto. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizaremos el texto del anterior taller para realizar este punto."
      ],
      "metadata": {
        "id": "xTd3zPbTFBUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "          \"Nosotros siempre venimos a París\",\n",
        "          \"El profesor es de Australia\",\n",
        "          \"Yo vivo en Bogotá\",\n",
        "          \"Él viene de Taiwán\",\n",
        "          \"La capital de Turquía es Ankara\"\n",
        "         ]"
      ],
      "metadata": {
        "id": "q17RowlNtbge"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordemos que la función de preprocesamiento que usaremos para generar nuestros ejemplos de entrenamiento.\n",
        "Es decir, ponemos las letras en minúsculas, quitamos tildes y luego tokenizamos las palabras."
      ],
      "metadata": {
        "id": "HbnSi2WDFsU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quitartildes(s):\n",
        "    # -> NFD y eliminar diacríticos\n",
        "    s = re.sub(\n",
        "            r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", \n",
        "            normalize( \"NFD\", s), 0, re.I\n",
        "        )\n",
        "\n",
        "    # -> NFC\n",
        "    return normalize( 'NFC', s)\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "  return quitartildes(sentence).lower().split()\n",
        "\n",
        "# Crea nuestro conjunto de entrenamiento\n",
        "train_sentences = [preprocess_sentence(sent) for sent in corpus]\n",
        "train_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag_N7XLJteAR",
        "outputId": "fe5a10ad-4734-4e3e-c403-9f5b3a463df3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['nosotros', 'siempre', 'venimos', 'a', 'paris'],\n",
              " ['el', 'profesor', 'es', 'de', 'australia'],\n",
              " ['yo', 'vivo', 'en', 'bogota'],\n",
              " ['el', 'viene', 'de', 'taiwan'],\n",
              " ['la', 'capital', 'de', 'turquia', 'es', 'ankara']]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A diferencia del taller anterior, agregamos un nuevo target para los labels. \n",
        "\n",
        "Tenemos `locations` y `verbs`, en donde si la palabra corresponde a la primera generará `1` y `2` si es un verbo. De otra manera, generará `0`."
      ],
      "metadata": {
        "id": "q27vefTRF6Xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "locations = set([\"australia\", \"ankara\", \"paris\", \"bogota\", \"taiwan\", \"turquia\"])\n",
        "verbs = set(['venimos', 'es', 'vivo', 'viene', 'es'])"
      ],
      "metadata": {
        "id": "3kqgYeg3xfYT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_to_ix = {'other': 0, 'location': 1, 'verb':2}"
      ],
      "metadata": {
        "id": "4BD-tBJQt-LP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = []\n",
        "\n",
        "for i in train_sentences:\n",
        "  part = []\n",
        "  for j in i:\n",
        "    if j in locations:\n",
        "      part.append('location')\n",
        "    elif j in verbs:\n",
        "      part.append('verb')\n",
        "    else:\n",
        "      part.append('other')\n",
        "  train_labels.append(part)"
      ],
      "metadata": {
        "id": "a4TQCAhQ63Ib"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [(train_sentences[i],train_labels[i]) for i in range(len(corpus))]"
      ],
      "metadata": {
        "id": "pg65k-qywTVb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHR6bekqy2jL",
        "outputId": "2163ba47-b39d-4a4f-bafc-b0e83ec08878"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['nosotros', 'siempre', 'venimos', 'a', 'paris'],\n",
              "  ['other', 'other', 'verb', 'other', 'location']),\n",
              " (['el', 'profesor', 'es', 'de', 'australia'],\n",
              "  ['other', 'other', 'verb', 'other', 'location']),\n",
              " (['yo', 'vivo', 'en', 'bogota'], ['other', 'verb', 'other', 'location']),\n",
              " (['el', 'viene', 'de', 'taiwan'], ['other', 'verb', 'other', 'location']),\n",
              " (['la', 'capital', 'de', 'turquia', 'es', 'ankara'],\n",
              "  ['other', 'other', 'other', 'location', 'verb', 'location'])]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set(w for s in train_sentences for w in s)\n",
        "vocabulary.add(\"<unk>\")\n",
        "vocabulary.add(\"<pad>\")"
      ],
      "metadata": {
        "id": "j90W79TVuqm2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ix_to_word = sorted(list(vocabulary))"
      ],
      "metadata": {
        "id": "KQr3Ku1_uxAZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_ix = {}\n",
        "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
        "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index"
      ],
      "metadata": {
        "id": "Pvno95UX3BKA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos la red neuronal."
      ],
      "metadata": {
        "id": "01_hwygBGmAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] if w in to_ix else to_ix[\"<unk>\"] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)"
      ],
      "metadata": {
        "id": "WQYg5c2ethV2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "PkQ2io7JolTq"
      },
      "outputs": [],
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea el  nuevo modelo con la función de pérdida de `NLLoss`(Negative Log Likelihood) y nuevamente el optimizador `Adam`."
      ],
      "metadata": {
        "id": "FPsOQ_0UHRQj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "duOObhX9olTr"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 20\n",
        "HIDDEN_DIM = 10\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "        # Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        #  calling optimizer.step()\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# See what the scores are after training\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    print(tag_scores)\n",
        "    print(prepare_sequence(training_data[0][1],tag_to_ix))\n",
        "    print(torch.argmax(tag_scores, dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bzUOOcTvpM0",
        "outputId": "b8d468d6-f4f5-4a87-8aa6-e637ad7639c4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0371, -5.1502, -3.4870],\n",
            "        [-0.0262, -7.0279, -3.6912],\n",
            "        [-3.8552, -4.4633, -0.0332],\n",
            "        [-0.0118, -4.6812, -6.0058],\n",
            "        [-5.0651, -0.0182, -4.4445]])\n",
            "tensor([0, 0, 2, 0, 1])\n",
            "tensor([0, 0, 2, 0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nota: puede usar las herramientas de pytorch para crear el vocabulario, y las herramientas de relleno (pad)"
      ],
      "metadata": {
        "id": "BLhSLfBV1K6x"
      }
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "9fdc09349c2f513784916d4df172f4ad7505cb7cc6e6b83acb68c78efe09a6d0"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Taller4_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}