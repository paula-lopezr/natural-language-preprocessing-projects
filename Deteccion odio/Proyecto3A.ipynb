{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017037f5-05b7-4e53-b46e-55c49de92d22",
   "metadata": {
    "id": "017037f5-05b7-4e53-b46e-55c49de92d22"
   },
   "source": [
    "# Proyecto 3: Detección de Odio y Generando Alegría"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e32eda",
   "metadata": {
    "id": "73e32eda"
   },
   "source": [
    "Este proyecto tiene como objetivo que aprendan a utilizar los transformers en aplicaciones de la vida real, usando el ecosistema de https://huggingface.co/ (librerías de transformers, datasets, tokenizers, etc) y PyTorch. \\\n",
    "\n",
    "Utilizarán como base un transformer derivado de BERT llamado [RoBERTuito](https://huggingface.co/pysentimiento/robertuito-base-uncased) que ha sido entrenado (en español) sobre más de 500 millones de tweets.  El paper de dicho modelo lo pueden encontrar en este [link](https://arxiv.org/abs/2111.09453)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509acfeb",
   "metadata": {
    "id": "509acfeb"
   },
   "source": [
    "### Parte 1. Detección de Odio\n",
    "\n",
    "El discurso del Odio, lamentablemente se esta volviendo muy frecuente, e inclusive se pueden programar bots para promoverlo. En esta parte utilizarán un modelo de HF llamado [robertuito-hate-speech](https://huggingface.co/pysentimiento/robertuito-hate-speech). La idea es que utilicen la API de Twitter que usaron en el primer proyecto para recopilar una gran cantidad de tweets (durante la última semana/día(s) en **Colombia**) y detecten cuales de esos tweets están siendo \"odiosos\". Dicho modelo produce tres etiquetas (es multi-label): HS-Hate Speach; TR - targeted to specific individual y; AG - Aggressive. Una probabilidad relativamente alta en cualquier de esas categorías indica que es un tweet de odio.\n",
    "\n",
    "Este problema no requiere ningún entrenamiento, es sólo usar el modelo de transformer mencionado. \n",
    "\n",
    "¿Qué usuarios en los últimos días estan frecuentemente usando un discurso de odio? ¿Serán Bots?\n",
    "\n",
    "Generé un pipeline (una función, script, etc.) que pueda variar el tiempo de recopilación y así poder determinar diferentes momentos en los que se esta hablando de esta forma. \n",
    "\n",
    "Para esta parte les puede ser útil la documentación de HF o el capítulo 2 y 3 del libro de [Natural Language Processing with Transformers](https://github.com/nlp-with-transformers/notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0ba7c",
   "metadata": {
    "id": "49e0ba7c"
   },
   "source": [
    "### Parte 2. Contrarestar el Odio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f61c82",
   "metadata": {
    "id": "39f61c82"
   },
   "source": [
    "Como seguramente se esta generando odio a través de tweets  el objetivo de esta parte es crear un bot que genere un discurso de alegría.\n",
    "\n",
    "RoBERTuito también tiene un modelo para detección de emociones: [robertuito-emotion-analysis](https://huggingface.co/pysentimiento/robertuito-emotion-analysis). La idea es que usen una estrategia similar a la de la parte 1 para identificar que tweets en español son los que producen la etiqueta \"joy\", durante la última semana (no necesariamente limitado a Colombia). Guarde estos tweets porque serviran para realizar un generador de alegría. \n",
    "\n",
    "Ahora, deberán usar un modelo de transformer basado en GPT2 (por ejemplo: datificate/gpt2-small-spanish, PlanTL-GOB-ES/gpt2-large-bne, DeepESP/gpt2-spanish) y realizar un fine-tuning en la generación del texto sobre el dataset de alegría conseguido anteriormente (use muy pocos epochs, 2 o 3). Evalúe los resultados sobre el conjunto de validación usando una métrica para modelo de lenguaje (por ejemplo Perplexity). \n",
    "\n",
    "Una vez obtenido el modelo de generación llevelo a producción con una de las dos formas siguientes: (1) realice una interfaz para la generación del texto o (2) use la API de twitter para realizarlo de forma automática.\n",
    "\n",
    "Para éste punto recomiendo revisar lo siguiente:\n",
    "\n",
    "[1] Capítulo 5 del libro de [Natural Language Processing with Transformers](https://github.com/nlp-with-transformers/notebooks)\n",
    "\n",
    "[2] [Este notebook sobre generación de texto](https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)\n",
    "\n",
    "[2] [Este notebook sobre fine-tune un modelo de generación de texto](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c084bfb2-c0ab-4dad-9c12-a432cbc5b09f",
   "metadata": {
    "id": "c084bfb2-c0ab-4dad-9c12-a432cbc5b09f"
   },
   "source": [
    "-----\n",
    "Es importante que realicen gráficos y visualizaciones que ayuden a la interpretación. No olviden ir analizando y comentando los hallazgos, y sobretodo **concluir**. El entregable es un notebook de Jupyter, debidamente presentado y comentado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edec9558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.18.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: requests in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers) (4.62.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers) (3.10.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata->transformers) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->transformers) (2.0.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\paula\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37649e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypensamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "C8p1OxQAlpUF",
   "metadata": {
    "id": "C8p1OxQAlpUF"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pysentimiento'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16404/3284621906.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpysentimiento\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocess_tweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pysentimiento'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoConfig\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import requests\n",
    "import dateutil.parser\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset, ClassLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nhVB8QEqruNZ",
   "metadata": {
    "id": "nhVB8QEqruNZ"
   },
   "outputs": [],
   "source": [
    "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAALLsZQEAAAAA9v%2FmbMJNGcyffUFN0jN%2FVs%2Bnjg8%3DMFhv6aHFDFZK5k5ln0N3kqqcvFIqtMCk7tZZJlzgCMsFmTPu9C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "azT444NWroOq",
   "metadata": {
    "id": "azT444NWroOq"
   },
   "outputs": [],
   "source": [
    "def auth():\n",
    "    return os.getenv('TOKEN')\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "def create_url(keyword, start_date, end_date, max_results = 10):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/recent\" #Change to the endpoint you want to collect data from\n",
    "    # search_url = \"https://api.twitter.com/2/tweets/search/all\" # With an academic research access\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword,\n",
    "                    'start_time': start_date,\n",
    "                    'end_time': end_date,\n",
    "                    'max_results': max_results,\n",
    "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                    'next_token': {}}\n",
    "    return (search_url, query_params)\n",
    "\n",
    "def connect_to_endpoint(url, headers, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
    "    # print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0-HuSZKNsF_C",
   "metadata": {
    "id": "0-HuSZKNsF_C"
   },
   "outputs": [],
   "source": [
    "bearer_token = auth()\n",
    "headers = create_headers(bearer_token)\n",
    "\n",
    "start_time = \"2022-04-21T00:00:00.000Z\"\n",
    "end_time = \"2022-04-26T00:00:00.000Z\"\n",
    "max_results = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fqk3meX1sPno",
   "metadata": {
    "id": "fqk3meX1sPno"
   },
   "outputs": [],
   "source": [
    "def append_to_csv(json_response, fileName):\n",
    "\n",
    "    #A counter variable\n",
    "    counter = 0\n",
    "\n",
    "    #Open OR create the target CSV file\n",
    "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Loop through each tweet\n",
    "    for tweet in json_response['data']:\n",
    "        \n",
    "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
    "        # So we will account for that\n",
    "\n",
    "        # 1. Author ID\n",
    "        author_id = tweet['author_id']\n",
    "\n",
    "        # 2. Time created\n",
    "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
    "\n",
    "        # 3. Geolocation\n",
    "        if ('geo' in tweet):   \n",
    "            geo = tweet['geo']['place_id']\n",
    "        else:\n",
    "            geo = \" \"\n",
    "\n",
    "        # 4. Tweet ID\n",
    "        tweet_id = tweet['id']\n",
    "\n",
    "        # 5. Language\n",
    "        lang = tweet['lang']\n",
    "\n",
    "        # 6. Tweet metrics\n",
    "        retweet_count = tweet['public_metrics']['retweet_count']\n",
    "        reply_count = tweet['public_metrics']['reply_count']\n",
    "        like_count = tweet['public_metrics']['like_count']\n",
    "        quote_count = tweet['public_metrics']['quote_count']\n",
    "\n",
    "        # 7. source\n",
    "        source = tweet['source']\n",
    "\n",
    "        # 8. Tweet text\n",
    "        text = tweet['text']\n",
    "        \n",
    "        # Assemble all data in a list\n",
    "        res = [author_id, created_at, geo, tweet_id, lang, like_count, quote_count, reply_count, retweet_count, source, text]\n",
    "        \n",
    "        # Append the result to the CSV file\n",
    "        csvWriter.writerow(res)\n",
    "        counter += 1\n",
    "\n",
    "    # When done, close the CSV file\n",
    "    csvFile.close()\n",
    "\n",
    "    # Print the number of tweets for this iteration\n",
    "    # print(\"# of Tweets added from this response: \", counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ei5x9yXisQzq",
   "metadata": {
    "id": "Ei5x9yXisQzq"
   },
   "outputs": [],
   "source": [
    "def json_csv(keyword, name):\n",
    "    #Inputs for tweets\n",
    "    bearer_token = auth()\n",
    "    headers = create_headers(bearer_token)\n",
    "    keyword = keyword\n",
    "    start_list =    ['2022-05-01T00:00:00.000Z',\n",
    "                    '2022-05-02T00:00:00.000Z',\n",
    "                     '2022-05-03T00:00:00.000Z',\n",
    "                     '2022-05-04T00:00:00.000Z']\n",
    "\n",
    "    end_list =      ['2022-05-03T00:00:00.000Z',\n",
    "                    '2022-05-04T00:00:00.000Z',\n",
    "                     '2022-05-05T00:00:00.000Z',\n",
    "                     '2022-05-06T00:00:00.000Z']\n",
    "    max_results = 100\n",
    "\n",
    "    #Total number of tweets we collected from the loop\n",
    "    total_tweets = 0\n",
    "\n",
    "    # Create file\n",
    "    csvFile = open(name+'.csv', \"a\", newline=\"\", encoding='utf-8')\n",
    "    csvWriter = csv.writer(csvFile)\n",
    "\n",
    "    #Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "    csvWriter.writerow(['author id', 'created_at', 'geo', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet'])\n",
    "    csvFile.close()\n",
    "\n",
    "    for i in range(0,len(start_list)):\n",
    "\n",
    "        # Inputs\n",
    "        count = 0 # Counting tweets per time period\n",
    "        max_count = 500 # Max tweets per time period\n",
    "        flag = True\n",
    "        next_token = None\n",
    "        \n",
    "        # Check if flag is true\n",
    "        while flag:\n",
    "            # Check if max_count reached\n",
    "            if count >= max_count:\n",
    "                break\n",
    "            # print(\"-------------------\")\n",
    "            # print(\"Token: \", next_token)\n",
    "            url = create_url(keyword, start_list[i],end_list[i], max_results)\n",
    "            json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
    "            result_count = json_response['meta']['result_count']\n",
    "\n",
    "            if 'next_token' in json_response['meta']:\n",
    "                # Save the token to use for next call\n",
    "                next_token = json_response['meta']['next_token']\n",
    "                # print(\"Next Token: \", next_token)\n",
    "                if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                    # print(\"Start Date: \", start_list[i])\n",
    "                    append_to_csv(json_response, name+\".csv\")\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    # print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    # print(\"-------------------\")\n",
    "                    time.sleep(1)                \n",
    "            # If no next token exists\n",
    "            else:\n",
    "                if result_count is not None and result_count > 0:\n",
    "                    # print(\"-------------------\")\n",
    "                    # print(\"Start Date: \", start_list[i])\n",
    "                    append_to_csv(json_response, name+\".csv\")\n",
    "                    count += result_count\n",
    "                    total_tweets += result_count\n",
    "                    # print(\"Total # of Tweets added: \", total_tweets)\n",
    "                    # print(\"-------------------\")\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                #Since this is the final request, turn flag to false to move to the next time period.\n",
    "                flag = False\n",
    "                next_token = None\n",
    "            time.sleep(1)\n",
    "    # print(\"Total number of results: \", total_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VYQP_GcSsYOy",
   "metadata": {
    "id": "VYQP_GcSsYOy"
   },
   "outputs": [],
   "source": [
    "keyword = 'colombia lang:es -is:retweet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k-MF2M7qsk4j",
   "metadata": {
    "id": "k-MF2M7qsk4j"
   },
   "outputs": [],
   "source": [
    "json_csv(keyword, 'tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QSqzhKAlsnyC",
   "metadata": {
    "id": "QSqzhKAlsnyC"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162798f",
   "metadata": {
    "id": "5162798f"
   },
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df)\n",
    "ds = ds.rename_columns({'tweet':'text', 'author id':'author_id'},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KV64tamfzkyd",
   "metadata": {
    "id": "KV64tamfzkyd"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "tokenizer = AutoTokenizer.from_pretrained('pysentimiento/robertuito-hate-speech')\n",
    "model = (AutoModelForSequenceClassification.from_pretrained('pysentimiento/robertuito-hate-speech')\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e5c4fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "872848d3ac59482cbee94c56642ababe",
      "df8ea4eaf7434d2783ef9853e2d6bb2d",
      "00dd55e718c74dde8715f508d81e706d",
      "238f0997db6849ebaef2529defa09d27",
      "ee8f780d0231418ca182e32eb1ce2a67",
      "cf1fa23dff5b44b089f5b8539adf1b0a",
      "495cae68d79a4f819115e811fe2a5969",
      "b83ed9f09fac48f79f05399dc4bc09b8",
      "d56b01e0e88848e694774d8944dedcce",
      "27bef858bd014a1f930e8899f94aa2ee",
      "517be77f8fd54edda03e1d33fbea2892",
      "8b060b3801ac491ba0a9246c02ba044a",
      "ab7b6200c94b444b9e2ee56e0a501af6",
      "e6d397411c594d3192046fe933abeeb9",
      "362996f330ae4670988f274fc0747535",
      "50830ac7357e42a1bc191df910e95c75",
      "9f65197d5b204ec9b5a739d6cffc029d",
      "c434739494784d26b6f9a9ec87cbeede",
      "aade0530bfa44f5890416a2b9a594469",
      "3204ca897ea244e392f0f15b59254e63",
      "bd6642f941bf4790b2934c558af9edde",
      "d3dc0bdac2c94182adbd6592e516594f"
     ]
    },
    "id": "41e5c4fe",
    "outputId": "361b0ea5-0b36-4f0b-830f-3925a8125a43"
   },
   "outputs": [],
   "source": [
    "# Dataset preprocesado \n",
    "ds_pre = ds.map(lambda x: {'text':preprocess_tweet(x['text'])})\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)\n",
    "\n",
    "ds_pre = ds_pre.map(tokenize,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef946fd",
   "metadata": {
    "id": "0ef946fd"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                   per_device_eval_batch_size=64)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adadb48a",
   "metadata": {
    "id": "adadb48a"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model= model.to(device), \n",
    "    args=training_args,\n",
    "    train_dataset=ds_pre,\n",
    "    eval_dataset=ds_pre,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a580b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "967a580b",
    "outputId": "107766dd-2a39-44e1-99dd-4374425a2e96"
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"pysentimiento/robertuito-hate-speech\")\n",
    "config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117ac745",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "117ac745",
    "outputId": "27c526d8-2e91-449f-97bb-d008d6be2101",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: retweet_count, lang, id, source, author_id, reply_count, quote_count, like_count, geo, created_at, text. If retweet_count, lang, id, source, author_id, reply_count, quote_count, like_count, geo, created_at, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6852\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='91' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 91/108 33:12 < 06:16, 0.05 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bc84476d493c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_pre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2404\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m         output = eval_loop(\n\u001b[0;32m-> 2406\u001b[0;31m             \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2407\u001b[0m         )\n\u001b[1;32m   2408\u001b[0m         \u001b[0mtotal_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_batch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2508\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m             \u001b[0minputs_decode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_inputs_for_metrics\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   2757\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2758\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2759\u001b[0;31m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2760\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2761\u001b[0m                         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m         )\n\u001b[1;32m   1216\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         )\n\u001b[1;32m    847\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 190\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2484\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         )\n\u001b[0;32m-> 2486\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(ds_pre)\n",
    "pred_labels = [config.id2label[x] for x in preds.predictions.argmax(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf492b",
   "metadata": {
    "id": "3faf492b"
   },
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd5cf2",
   "metadata": {
    "id": "91bd5cf2"
   },
   "outputs": [],
   "source": [
    "# pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jhT4FOaD6IjH",
   "metadata": {
    "id": "jhT4FOaD6IjH"
   },
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EJprUTQPNhJe",
   "metadata": {
    "id": "EJprUTQPNhJe"
   },
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q tensorflow==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "kLYcWkIx2RgK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kLYcWkIx2RgK",
    "outputId": "dff23d6d-d012-4cbd-a907-5de1fa87be0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 497.5 MB 19 kB/s \n",
      "\u001b[?25hCollecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.25.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 40.8 MB/s \n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting six>=1.12.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.46.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 54.5 MB/s \n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 43.3 MB/s \n",
      "\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 50.0 MB/s \n",
      "\u001b[?25hCollecting libclang>=9.0.1\n",
      "  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 43.6 MB/s \n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 4.2 MB/s \n",
      "\u001b[?25hCollecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 58.7 MB/s \n",
      "\u001b[?25hCollecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 4.1 MB/s \n",
      "\u001b[?25hCollecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.5 MB/s \n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 5.7 MB/s \n",
      "\u001b[?25hCollecting numpy>=1.20\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 34.4 MB/s \n",
      "\u001b[?25hCollecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 6.4 MB/s \n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 54.9 MB/s \n",
      "\u001b[?25hCollecting setuptools\n",
      "  Using cached setuptools-62.1.0-py3-none-any.whl (1.1 MB)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 53.7 MB/s \n",
      "\u001b[?25hCollecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 7.6 MB/s \n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 51.7 MB/s \n",
      "\u001b[?25hCollecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
      "\u001b[K     |████████████████████████████████| 224 kB 31.9 MB/s \n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 57.6 MB/s \n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 29.4 MB/s \n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 59.6 MB/s \n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 6.0 MB/s \n",
      "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 7.8 MB/s \n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 75.3 MB/s \n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "\u001b[K     |████████████████████████████████| 149 kB 48.4 MB/s \n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 55.1 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=73f397bebfd750ad92a7bedd3a7620c41a83bc910de32c8b7934bb5ccee87e84\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built termcolor\n",
      "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, typing-extensions, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, setuptools, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, cached-property, absl-py, wrapt, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
      "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "google-api-core 1.31.5 requires google-auth<2.0dev,>=1.25.0, but you have google-auth 2.6.6 which is incompatible.\n",
      "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.1.2 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-5.0.0 certifi-2021.10.8 charset-normalizer-2.0.12 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.46.0 h5py-3.6.0 idna-3.3 importlib-metadata-4.11.3 keras-2.8.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 numpy-1.21.6 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 setuptools-62.1.0 six-1.16.0 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.2.0 urllib3-1.26.9 werkzeug-2.1.2 wheel-0.37.1 wrapt-1.14.1 zipp-3.8.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "absl",
         "astunparse",
         "cached_property",
         "certifi",
         "charset_normalizer",
         "gast",
         "google",
         "h5py",
         "idna",
         "importlib_metadata",
         "keras_preprocessing",
         "numpy",
         "opt_einsum",
         "pkg_resources",
         "pyasn1",
         "pyasn1_modules",
         "requests",
         "six",
         "tensorboard",
         "tensorflow",
         "tensorflow_estimator",
         "termcolor",
         "urllib3",
         "werkzeug",
         "wrapt",
         "zipp"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --ignore-installed --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "NFmEQ2Fz8zYI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "NFmEQ2Fz8zYI",
    "outputId": "307a968d-9e41-44d2-fc3e-d8328599caaa"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import GPT2Tokenizer\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "from transformers import TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "q5JWZijtCzPO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 983
    },
    "id": "q5JWZijtCzPO",
    "outputId": "be5a68e0-b9ea-4f58-8ca4-77258d0a28c0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf3c6b2cb324503a514d62a29e20b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ff6000d6464ef9959f1fd6695ca2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cb2eb69770406db1f89a7766ece0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acef643ed994844b60c43373f1ac69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/475M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1zC2Nqx0DFQE",
   "metadata": {
    "id": "1zC2Nqx0DFQE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "We are happy to be here with you. We are here to help you. We are here to help you. We are here to help you. We are here to help you. We are here to help you. We are here to help you\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('We are happy to be here', return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "WhzElz8VFTM6",
   "metadata": {
    "id": "WhzElz8VFTM6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: We are happy to be here to provide you with a great experience. I hope you're as excited for your next game as I am, that you enjoy it, and that you enjoy doing this for me.\n",
      "\n",
      "I will be making this part\n",
      "1: We are happy to be here to help you understand how a great community can help you live up to your mission, while protecting what makes your community unique.\n",
      "\n",
      "Learn More\n",
      "\n",
      "Become a member of the Redwood Forest Community\n",
      "\n",
      "Red\n",
      "2: We are happy to be here with you.\" \"Oh...okay?\" \"Well...you will have my attention for a while.\" \"I will keep you there, just in case.\" \"That'll do, isn't it?\" \"Oh no\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=50, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WPKG3uCESaet",
   "metadata": {
    "id": "WPKG3uCESaet"
   },
   "source": [
    "language_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5u8lxHBUQpzT",
   "metadata": {
    "id": "5u8lxHBUQpzT"
   },
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tbMqgqwFRLwQ",
   "metadata": {
    "id": "tbMqgqwFRLwQ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16404/3391387485.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshow_random_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "show_random_elements(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IJ-fpCtiTdPA",
   "metadata": {
    "id": "IJ-fpCtiTdPA"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i-sHIQirVlUi",
   "metadata": {
    "id": "i-sHIQirVlUi"
   },
   "outputs": [],
   "source": [
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenized_datasets = ds.map(tokenize, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7sJC5fjcdt1",
   "metadata": {
    "id": "b7sJC5fjcdt1"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vlpdMAScZrFr",
   "metadata": {
    "id": "vlpdMAScZrFr"
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y0eWph9Og97B",
   "metadata": {
    "id": "Y0eWph9Og97B"
   },
   "source": [
    "No funciona este entonces hay que depurar los datos de manera que, todos los datoq ue se metan solo sea el texto y ya \n",
    "\n",
    "pero hay que hacerlo con dataset y no pude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oAJC0JE_Z375",
   "metadata": {
    "id": "oAJC0JE_Z375"
   },
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CPVXvZxOhJhV",
   "metadata": {
    "id": "CPVXvZxOhJhV"
   },
   "outputs": [],
   "source": [
    "txt = ds['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9S8d9CJBht1e",
   "metadata": {
    "id": "9S8d9CJBht1e"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4629475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34671cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Proyecto3A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "es",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "es",
   "useGoogleTranslate": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00dd55e718c74dde8715f508d81e706d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b83ed9f09fac48f79f05399dc4bc09b8",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d56b01e0e88848e694774d8944dedcce",
      "value": 1
     }
    },
    "238f0997db6849ebaef2529defa09d27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27bef858bd014a1f930e8899f94aa2ee",
      "placeholder": "​",
      "style": "IPY_MODEL_517be77f8fd54edda03e1d33fbea2892",
      "value": " 13743/? [00:04&lt;00:00, 3064.46ex/s]"
     }
    },
    "27bef858bd014a1f930e8899f94aa2ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3204ca897ea244e392f0f15b59254e63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "362996f330ae4670988f274fc0747535": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd6642f941bf4790b2934c558af9edde",
      "placeholder": "​",
      "style": "IPY_MODEL_d3dc0bdac2c94182adbd6592e516594f",
      "value": " 14/14 [00:02&lt;00:00,  5.68ba/s]"
     }
    },
    "495cae68d79a4f819115e811fe2a5969": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "50830ac7357e42a1bc191df910e95c75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "517be77f8fd54edda03e1d33fbea2892": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "872848d3ac59482cbee94c56642ababe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df8ea4eaf7434d2783ef9853e2d6bb2d",
       "IPY_MODEL_00dd55e718c74dde8715f508d81e706d",
       "IPY_MODEL_238f0997db6849ebaef2529defa09d27"
      ],
      "layout": "IPY_MODEL_ee8f780d0231418ca182e32eb1ce2a67"
     }
    },
    "8b060b3801ac491ba0a9246c02ba044a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ab7b6200c94b444b9e2ee56e0a501af6",
       "IPY_MODEL_e6d397411c594d3192046fe933abeeb9",
       "IPY_MODEL_362996f330ae4670988f274fc0747535"
      ],
      "layout": "IPY_MODEL_50830ac7357e42a1bc191df910e95c75"
     }
    },
    "9f65197d5b204ec9b5a739d6cffc029d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aade0530bfa44f5890416a2b9a594469": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab7b6200c94b444b9e2ee56e0a501af6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f65197d5b204ec9b5a739d6cffc029d",
      "placeholder": "​",
      "style": "IPY_MODEL_c434739494784d26b6f9a9ec87cbeede",
      "value": "100%"
     }
    },
    "b83ed9f09fac48f79f05399dc4bc09b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "bd6642f941bf4790b2934c558af9edde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c434739494784d26b6f9a9ec87cbeede": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf1fa23dff5b44b089f5b8539adf1b0a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3dc0bdac2c94182adbd6592e516594f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d56b01e0e88848e694774d8944dedcce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "df8ea4eaf7434d2783ef9853e2d6bb2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf1fa23dff5b44b089f5b8539adf1b0a",
      "placeholder": "​",
      "style": "IPY_MODEL_495cae68d79a4f819115e811fe2a5969",
      "value": ""
     }
    },
    "e6d397411c594d3192046fe933abeeb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aade0530bfa44f5890416a2b9a594469",
      "max": 14,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3204ca897ea244e392f0f15b59254e63",
      "value": 14
     }
    },
    "ee8f780d0231418ca182e32eb1ce2a67": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
